{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMRKpQz9XTP4krKZiWIenw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peppopi/Piratas/blob/main/%22_safetensors%22_Inspector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2qbYmod_sf_",
        "outputId": "a9907063-2704-4929-9eb4-4a749093375d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title ## üè¥‚Äç‚ò†Ô∏è ¬°Drive Mount!\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## The program allows you to view the tags of any safestensor file, sort them by frequency, and display the corresponding activation tag.\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def process_file(file, encoding, tag_count):\n",
        "    with open(file, 'r', encoding=encoding) as f:\n",
        "        tag_content = f.read()\n",
        "\n",
        "    # Search for the content between the quotes of the \"ss_tag_frequency\" tag\n",
        "    match = re.search(r'\"ss_tag_frequency\":\"({.+?})\"', tag_content)\n",
        "    if match is None:\n",
        "        print(\"The 'ss_tag_frequency' tag was not found in the file.\")\n",
        "        return\n",
        "\n",
        "    tag_content = match.group(1)\n",
        "\n",
        "    # Extract the tag-frequency pairs using regular expressions\n",
        "    pairs = re.findall(r'\"([^\"]+)\": (\\d+)', tag_content)\n",
        "\n",
        "    # Create a list of dictionaries with the tag data\n",
        "    data_list = [{'Tag': tag, 'Frequency': int(frequency)} for tag, frequency in pairs]\n",
        "\n",
        "    # Create a DataFrame from the list of dictionaries\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    # Sort the tags by frequency in descending order\n",
        "    df = df.sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "    # Display the first \"tag_count\" complete tags and their frequencies\n",
        "    pd.set_option('display.max_rows', tag_count)\n",
        "    print(df.head(tag_count))\n",
        "\n",
        "\n",
        "file_name = '/content/drive/MyDrive/Dataset/Pruebas/TheOrb-05.safetensors'   # Replace '/content/drive/MyDrive/Dataset/aeromorphAlo-03.safetensors' with the correct path to your file\n",
        "encoding = 'latin-1'  # Replace 'utf-8' with the correct encoding of your file\n",
        "tag_count = 50 + 1  # Define the number of tags to display by replacing 50 with the desired value\n",
        "\n",
        "process_file(file_name, encoding, tag_count)\n"
      ],
      "metadata": {
        "id": "5zRXp9TUASCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## The second program provides all the metadata/hyperparameters of the safestensor and presents it in an executive summary.\n",
        "def generate_report_from_file(file_name, encoding):\n",
        "    with open(file_name, 'r', encoding=encoding) as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Analyze the content to extract the necessary values\n",
        "    value1 = get_value(content, \"ss_sd_model_name\")\n",
        "    value2 = get_value(content, \"ss_clip_skip\")\n",
        "    value3 = get_value(content, \"ss_num_train_images\")\n",
        "    value4 = get_value(content, \"ss_tag_frequency\")  # Replace with the correct key based on the data structure\n",
        "    value5 = get_value(content, \"ss_epoch\")\n",
        "    value6 = get_value(content, \"ss_face_crop_aug_range\")\n",
        "    value7 = get_value(content, \"ss_full_fp16\")\n",
        "    value8 = get_value(content, \"ss_gradient_accumulation_steps\")\n",
        "    value9 = get_value(content, \"ss_gradient_checkpointing\")\n",
        "    value10 = get_value(content, \"ss_learning_rate\")\n",
        "    value11 = get_value(content, \"ss_lowram\")\n",
        "    value12 = get_value(content, \"ss_lr_scheduler\")\n",
        "    value13 = get_value(content, \"ss_lr_warmup_steps\")\n",
        "    value14 = get_value(content, \"ss_max_grad_norm\")\n",
        "    value15 = get_value(content, \"ss_max_token_length\")\n",
        "    value16 = get_value(content, \"ss_max_train_steps\")\n",
        "    value17 = get_value(content, \"ss_min_snr_gamma\")\n",
        "    value18 = get_value(content, \"ss_mixed_precision\")\n",
        "    value19 = get_value(content, \"ss_network_alpha\")\n",
        "    value20 = get_value(content, \"ss_network_dim\")\n",
        "    value21 = get_value(content, \"ss_network_module\")\n",
        "    value22 = get_value(content, \"ss_new_sd_model_hash\")\n",
        "    value23 = get_value(content, \"ss_noise_offset\")\n",
        "    value24 = get_value(content, \"ss_num_batches_per_epoch\")\n",
        "    value25 = get_value(content, \"ss_cache_latents\")\n",
        "    value26 = get_value(content, \"ss_caption_dropout_every_n_epochs\")\n",
        "    value27 = get_value(content, \"ss_caption_dropout_rate\")\n",
        "    value28 = get_value(content, \"ss_caption_tag_dropout_rate\")\n",
        "    value29 = get_value(content, \"ss_dataset_dirs\")  # Replace with the correct key based on the data structure\n",
        "    value30 = get_value(content, \"ss_num_epochs\")\n",
        "    value31 = get_value(content, \"ss_num_reg_images\")\n",
        "    value32 = get_value(content, \"ss_optimizer\")\n",
        "    value33 = get_value(content, \"ss_output_name\")\n",
        "    value34 = get_value(content, \"ss_prior_loss_weight\")\n",
        "    value35 = get_value(content, \"ss_sd_model_hash\")\n",
        "    value36 = get_value(content, \"ss_sd_scripts_commit_hash\")\n",
        "    value37 = get_value(content, \"ss_seed\")\n",
        "    value38 = get_value(content, \"ss_session_id\")\n",
        "    value39 = get_value(content, \"ss_text_encoder_lr\")\n",
        "    value40 = get_value(content, \"ss_unet_lr\")\n",
        "    value41 = get_value(content, \"ss_v2\")\n",
        "    value42 = get_value(content, \"sshs_legacy_hash\")\n",
        "    value43 = get_value(content, \"sshs_model_hash\")\n",
        "\n",
        "    # Generate the report using the extracted values\n",
        "    report = f'''Executive Summary Report:\n",
        "\n",
        "Important Parameters:\n",
        "- Model Name: {value1}\n",
        "- Clip Skip: {value2}\n",
        "- Number of Training Images: {value3}\n",
        "- Tag Frequency:\n",
        "- Epochs: {value5}\n",
        "- Face Crop Augmentation Range: {value6}\n",
        "- Full FP16: {value7}\n",
        "- Gradient Accumulation Steps: {value8}\n",
        "- Gradient Checkpointing: {value9}\n",
        "- Learning Rate: {value10}\n",
        "- Low RAM: {value11}\n",
        "- Learning Rate Scheduler: {value12}\n",
        "- Learning Rate Warmup Steps: {value13}\n",
        "- Max Gradient Norm: {value14}\n",
        "- Max Token Length: {value15}\n",
        "- Max Training Steps: {value16}\n",
        "- Min SNR Gamma: {value17}\n",
        "- Mixed Precision: {value18}\n",
        "- Network Alpha: {value19}\n",
        "- Network Dimension: {value20}\n",
        "- Network Module: {value21}\n",
        "- New SD Model Hash: {value22}\n",
        "- Noise Offset: {value23}\n",
        "- Number of Batches per Epoch: {value24}\n",
        "- Cache Latents: {value25}\n",
        "- Caption Dropout Every N Epochs: {value26}\n",
        "- Caption Dropout Rate: {value27}\n",
        "- Caption Tag Dropout Rate: {value28}\n",
        "- Number of Epochs: {value30}\n",
        "- Number of Regression Images: {value31}\n",
        "- Optimizer: {value32}\n",
        "- Output Name: {value33}\n",
        "- Prior Loss Weight: {value34}\n",
        "- SD Model Hash: {value35}\n",
        "- SD Scripts Commit Hash: {value36}\n",
        "- Seed: {value37}\n",
        "- Session ID: {value38}\n",
        "- Text Encoder Learning Rate: {value39}\n",
        "- UNet Learning Rate: {value40}\n",
        "- Version 2: {value41}\n",
        "- SSHS Legacy Hash: {value42}\n",
        "- SSHS Model Hash: {value43}'''\n",
        "\n",
        "    return report\n",
        "\n",
        "def get_value(content, key):\n",
        "    start = content.find(key) + len(key) + 3  # Add 3 to skip the characters ': \",'\n",
        "    end = content.find('\"', start)\n",
        "    value = content[start:end]\n",
        "    return value\n",
        "\n",
        "\n",
        "# Usage of the code to generate the report from a specific file\n",
        "file_name = '/content/drive/MyDrive/Dataset/Pruebas/TheOrb-05.safetensors'  # Replace '/path/to/file.txt' with the correct path to your file\n",
        "encoding = 'latin-1'  # Replace 'utf-8' with the correct encoding of your file\n",
        "generated_report = generate_report_from_file(file_name, encoding)\n",
        "print(generated_report)\n"
      ],
      "metadata": {
        "id": "urE2ENPpAm5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## The third program analyzes thousands of .safestensor files to obtain dataframes with data to scan it.\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def analyze_files_in_folder(folder, encoding):\n",
        "    # Get the list of files in the folder\n",
        "    files = os.listdir(folder)\n",
        "\n",
        "    # Create a list to store the data\n",
        "    data = []\n",
        "\n",
        "    # Iterate over the files\n",
        "    for file in files:\n",
        "        # Read the content of the file\n",
        "        file_path = os.path.join(folder, file)\n",
        "        with open(file_path, 'r', encoding=encoding) as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Generate the values for the executive summary\n",
        "        values = extract_values(content)\n",
        "\n",
        "        # Add the values to the list\n",
        "        for value_name, value_count in values.items():\n",
        "            current_value = {'File': file, 'Value Name': value_name, 'Value Count': value_count}\n",
        "            data.append(current_value)\n",
        "\n",
        "    # Create a DataFrame from the data\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Show the DataFrame with the values\n",
        "    print(df)\n",
        "\n",
        "def extract_values(content):\n",
        "    # Analyze the content to extract the necessary values [This is the only function you need to modify]\n",
        "    values = {}\n",
        "    value1 = get_value(content, \"ss_max_train_steps\")\n",
        "    value2 = get_value(content, \"ss_clip_skip\")\n",
        "    value3 = get_value(content, \"ss_num_train_images\")\n",
        "    value4 = get_value(content, \"ss_num_epochs\")\n",
        "    # Add more values as needed\n",
        "    values = {'ss_max_train_steps': value1, 'ss_clip_skip': value2, 'ss_num_train_images': value3, 'ss_num_epochs': value4}  # Update with the corresponding value names\n",
        "\n",
        "    return values\n",
        "\n",
        "def get_value(content, key):\n",
        "    start = content.find(key) + len(key) + 3  # Add 3 to skip the characters ': \",'\n",
        "    end = content.find('\"', start)\n",
        "    value = content[start:end]\n",
        "    return value\n",
        "\n",
        "# Specify the folder you want to analyze\n",
        "folder = '/content/drive/MyDrive/Dataset/Pruebas'\n",
        "encoding = 'latin-1'  # Replace 'utf-8' with the correct encoding of your files\n",
        "\n",
        "# Call the function to analyze the files in the folder\n",
        "analyze_files_in_folder(folder, encoding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bByc9Bw4A1-D",
        "outputId": "a17f5c33-6292-4208-b59f-64dec5ff5b73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          File           Value Name Value Count\n",
            "0  aeromorphAlo-03.safetensors   ss_max_train_steps        1890\n",
            "1  aeromorphAlo-03.safetensors         ss_clip_skip           2\n",
            "2  aeromorphAlo-03.safetensors  ss_num_train_images         378\n",
            "3  aeromorphAlo-03.safetensors        ss_num_epochs          10\n",
            "4        TheOrb-05.safetensors   ss_max_train_steps         550\n",
            "5        TheOrb-05.safetensors         ss_clip_skip           2\n",
            "6        TheOrb-05.safetensors  ss_num_train_images         110\n",
            "7        TheOrb-05.safetensors        ss_num_epochs          10\n"
          ]
        }
      ]
    }
  ]
}